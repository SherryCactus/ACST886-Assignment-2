---
title: "Assignment 2"
author: "Sherry Mu"
output:
  word_document: default
  html_document: default
---

#Question 1

Firstly, find out the date that each policyholder aged exactly 30 and then compare these dates with their date of death to make sure whether they are within the observation period.
```{r}
age30 <- function(DoB){
  DoB <- as.Date(DoB)
  d <- as.numeric(format(DoB, "%d"))
  m <- as.numeric(format(DoB, "%m"))
  y <- as.numeric(format(DoB, "%Y")) +30
  date <- as.Date(paste(y,m,d, sep = "/"))
  date
}

Dob <- c("1964/03/17", "1964/05/06", "1964/08/12", "1964/10/27", "1965/01/04", "1965/04/18", "1965/05/20", "1965/07/04", "1965/09/16", "1965/12/11")
Age <- age30(Dob)
Age
Death <- as.Date(c("1900/01/01", "1993/06/12", "1900/01/01", "1900/01/01", "1996/08/29", "1900/01/01", "1996/04/21", "1900/01/01", "1900/01/01", "1997/02/17"))
Withdrawal <- as.Date(c("1900/01/01", "1900/01/01", "1995/06/18", "1900/01/01", "1900/01/01", "1995/12/12", "1900/01/01", "1900/01/01", "1997/02/22", "1900/01/01"))
```

Then, find the age on the birthday nearest the policy issue date if withdrawal. Also, make sure that withdrawal happened is within the obsevation period.
```{r}
Age_Ends_Withdrawal <- as.Date(c("1900/01/01", "1900/01/01", "1995/12/18", "1900/01/01", "1900/01/01", "1996/06/16", "1900/01/01", "1900/01/01", "1996/08/22", "1900/01/01"))
Age_Starts_Death <- as.Date(c("1900/01/01", "1994/08/06", "1900/01/01", "1900/01/01", "1995/04/28", "1900/01/01", "1995/10/29", "1900/01/01", "1900/01/01", "1996/03/06"))
```

Finally, find the initial exposed to risk as follows.
```{r}
Days <- rep(0, 1, 10)
for(i in 1:10){
  if(Death[i] == "1900/01/01"){
    if(Withdrawal[i] == "1900/01/01"){
      Days[i] <- 365.25
    }else{
      n1 <- as.numeric(Age_Ends_Withdrawal[i])-as.numeric(Withdrawal[i])
      if(n1 > 0){
        d1 <- as.numeric(format(Age_Ends_Withdrawal[i], "%d"))
        m1 <- as.numeric(format(Age_Ends_Withdrawal[i], "%m"))
        y1 <- as.numeric(format(Age_Ends_Withdrawal[i], "%Y")) - 1
        date1 <- as.Date(paste(y1,m1,d1, sep = "-"))
        
        Days[i] <- as.numeric(Withdrawal[i]) - as.numeric(date1)
      }else{
        Days[i] <- 365.25
      }
    }
  }else{
    n2 <- as.numeric(Age_Starts_Death[i]) - as.numeric(Death[i])
    if(n2 > 0){
      Days[i] <- 0
    }else{
      if(n2 < 0){
        Days[i] <- 365.25
      }
    }
  }
}

Days

E <- sum(Days)/365.25
E
```

The observed number of death is
```{r}
d <- 2
```

Hence, by the method of moment, $q_{30}$ can be estimated by 
```{r}
qmom <- d/E
qmom
```

Since full data and constant force of mortality is assumed, the simplified binomial model can be used to calculate the MLE of $q_{30}$.
```{r}
N <- 10
for(i in 1:10){
  if(Death[i] == "1900/01/01"){
    N <- N-1
  }
}
N

qmle <- d/N
qmle
```

#Question 2

Firstly, insert and name the dataset
```{r}
Age <- c("40-44", "45-49", "50-54", "55-59", "60-64", "65-69", "70-74", "75-79", "80-84", "85-89", "90-94")
EtR <- c(15518, 19428, 21594, 21890, 19174, 15775, 11414, 6993, 3276, 1096, 201)
ActDeath <- c(65, 144, 219, 378, 465, 557, 685, 644, 471, 217, 67)
ExpDeath <- c(73.9, 134.6, 223.9, 346.3, 468.1, 600.2, 675.5, 637.4, 458.7, 240.6, 61.4)

qx_crude <- ActDeath/EtR
qx_grad <- ExpDeath/EtR
```

Check with smoothness
```{r}
D1 <- rep(0, 1, 10)
for(i in 1:10){
  D1[i] <- qx_grad[i+1]-qx_grad[i]
}
D1

D2 <- rep(0, 1, 9)
for(i in 1:9){
  D2[i] <- D1[i+1]-D1[i]
}
D2

D3 <- rep(0, 1, 8)
for(i in 1:8){
  D3[i] <- D2[i+1]-D2[i]
}
D3

for(i in 1:8){
  if(D3[i]*7^3 < qx_grad[i]){
    print("YES")
  }else{
    print("OOPS")
  }
}
```
It seems like the graduated mortality rate estimate does not have 3rd order smoothness.

Apply statistical tests at 0.05 significant level to compare the graduate estimates.
###Chi Square Test
```{r}
var_death <- EtR * qx_grad * (1-qx_grad)

z <- (ActDeath-ExpDeath)/sqrt(var_death)
z
chisq <- sum(z^2)
chisq
dchisq(chisq, df = length(z)-1)
```

Since it's greater than 0.05, we could not reject $H_0$ and adherence to data is not very poor.

###Standard Deviations Test
```{r}
Interval <- c("(-inf,-1)", "(-1,0)", "(0,1)", "(1,inf)")
E <- c(0.16*11, 0.34*11, 0.34*11, 0.16*11)
E
A <- c(3, 2, 5, 1)
stadev <- sum((A-E)^2/E)
stadev
dchisq(stadev, df = length(E)-1)
```

Since it's greater than 0.05, the null hypothesis is retained and adherence to data is sufficient.

Also, test someother properties of $z_i's$.
```{r}
n <- 0
for(i in 1:11){
  if(abs(z[i]) < 2/3){
    n <- n+1
  }
}
n
```

This indicates that half of the deviations fall in the interval $(-\frac{2}{3}, \frac{2}{3})$, which means the adherence to data is sufficient.

```{r}
pos <- 0
for(i in 1:11){
  if(z[i] >0){
    pos <- pos+1
  }
}
pos
neg <- 11-pos
neg

sort(z, decreasing = F)
```

This indicates that the number of positive and negative deviations are roughly equal. However, from the ascendinng order of the deviations we could see that the first 2 negatively large deviations are a bit offset by small deviations.

###Cumulative Deviations Test
```{r}
cumdev <- sum(ActDeath-ExpDeath)/sqrt(sum(var_death))
cumdev
```

Since the test statistic value is obviously smaller than 1.96, the critical value of a normal distribution at 5% significant level, the adherence to data is satisfactory. However, the test statistic is a bit small, which shows that the graduated estimates are too high compared to the data.

###Signs Test
```{r}
pbinom(0,11,0.5)
pbinom(1,11,0.5)
pbinom(2,11,0.5)
pbinom(3,11,0.5)

sign_critical <- 2
pos
```

Since the total number of $z_i's$ is small, the number of positive deviations will follow a Bin(11,0.5) distribution. The smallest possible value of k for $\sum_{i=0}^{k}(^m _j)0.5^{m} \ge \frac{0.05}{2}$ is 2, which is relatively smaller than the test statistic, 6. Hence we could not reject $H_0$ and adherence to data is sufficient. Also, this also indicates that the graduated estimates are a bit low compared to the data.

###Grouping of Signs Test
```{r}
grouppos <- 4
pf <- function(n1,n2,k){
  sum <- 0
  for(j in 1:k){
    sum <- sum + (factorial(n1-1) * factorial(n2+1) * factorial(n1)*factorial(n2))/(factorial(j-1)*factorial(n1-j) * factorial(j)*factorial(n2-j+1) * factorial(n1+n2))
  }
  sum
}

pf(pos, neg, 1)
pf(pos, neg, 2)
pf(pos, neg, 3)

grouppos_critical <- 2
```

Since the statistic, 4 distinct group of positive $z_i's$, is greater than the critical value, the smallest possible value of k for $\sum_{j=1}^{k}\frac{(^{n_1-1} _{j-1})(^{n_2+1} _{j})}{(^{n_1+n_2}) _{n_1}} \ge 0.05$ . Hence we can conclude that adherence to data is sufficient.

###Serial Correlations Test
```{r}
z1 <- sum(z[1:10]/10)
z1
z2 <- sum(z[2:11]/10)
z2

secor <- sum((z[1:10]-z1)*(z[2:11]-z2))*sqrt(10)/sqrt(sum((z[1:10]-z1)^2)*sum((z[2:11]-z2)^2))
secor
```

Since the test statistic is obviously smaller than 1.64, the $H_0$ is retained and adherence to data is sufficient. 

####In conclusion, according to the 6 tests above, the adherence to data is sufficient but the graduated estimates are a bit low compared to the data. 

#Question 3
###Tutorial 4 5.7

Let $D_x$ denote the rv total number of death. Since central exposed to risk is given, we assume it follows a Poisson distribution with mean $E^C_x\mu_x$ and the force of mortality is $\mu_x = Bc^x$ by using Gompertz' Law.

The likelihood function is
$$L = \prod_{x=70}^{75}\frac{e^{-E^C_x\mu_x}(E^C_x\mu_x)^{d_x}}{d_x!} = e^{-\sum_{x=70}^{75}E^C_x\mu_x}\prod_{x=70}^{75}\frac{(E^C_x\mu_x)^{d_x}}{d_x!}$$

Hence,substituting $\mu_x = Bc^x$ in, the log-likelihood function is
$$lnL = -\sum_{x=70}^{75}E^C_x\mu_x + \sum_{x=70}^{75}(d_xln(E^C_x\mu_x) - ln(d_x!)) = -\sum_{x=70}^{75}(E^C_xBc^x - d_xln(E^C_xBc^x) + ln(d_x!))$$

```{r, warning=FALSE}
x1 <- rep(70:75)
Ecx1 <- c(1000, 1005, 1010, 1008, 1006, 998)
dx1 <- c(80, 90, 95, 105, 115, 125)

f <- function(p){
  sum(Ecx1*p[1]*(p[2]^x1) - dx1*log(Ecx1*p[1]*(p[2]^x1)))
}
mle1 <- nlm(f,c(0.01, 0.01), hessian = T)
mle1
```

Hence the MLE of B and c are respectively
```{r}
B1 <- mle1$estimate[1]
c1 <- mle1$estimate[2]

mle1$estimate
```

The graduated estimates are
```{r}
grad1 <- B1*c1^x1
grad1
```


###Tutorial 4 5.8

For the weights $w_x$ set as $E_x$, we want to find parameters a and b by minimising $$\sum_{x=30}^{49}E_x(ln(\frac{q_x^c}{1-q_x^c})-(a+bx))^2$$, where $q_x^c$ denotes the crude estimates of $q_x$.
```{r}
x2 <- rep(30:49)
Ex2 <- c(70000, 66672, 68375, 65420, 61779, 66091, 68514, 69560, 65000, 66279, 67300, 65368, 65391, 62917, 66537, 62302, 62145, 63856, 61097, 61110)
dx2 <- c(39, 43, 34, 31, 23, 50, 48, 43, 48, 47, 62, 63, 84, 86, 120, 121, 122, 162, 151, 184)

qx2_crude <- dx2/Ex2
link2 <- log(qx2_crude/(1-qx2_crude))

f2 <- function(p){
  sum(Ex2 * (link2-p[1]-p[2]*x2)^2)
}

wls2 <- nlm(f2, c(0.001, 0.001))
wls2
```

Hence the weighted least squares estimates of a and b are respectively
```{r}
a2 <- wls2$estimate[1]
b2 <- wls2$estimate[2]

wls2$estimate
```

The graduated estimates are thus $q_x^g = \frac{e^{a+bx}}{1+e^{a+bx}}$
```{r}
grad2 <- exp(a2+b2*x2)/(1+exp(a2+b2*x2))
grad2
```

###Tutorial 4 5.9

Similar with above, we want to find parameters a and b by minimising $$\sum_{x=47}^{67}E_x(q_x^c-(a+bq_x^S))^2$$, where $q_x^c$ denotes the crude estimates of $q_x$.

```{r}
x3 <- rep(47:67)
Ex3 <- c(166, 187, 218, 243, 276, 302, 347, 390, 430, 494, 558, 628, 701, 813, 917, 1040, 1182, 1299, 1432, 1596, 1752)
dx3 <- c(2, 2, 4, 6, 2, 4, 7, 3, 9, 9, 8, 11, 14, 18, 18, 24, 30, 43, 41, 54, 64)
qsx3 <- c(0.00505, 0.00570, 0.00644, 0.00728, 0.00826, 0.00930, 0.01051, 0.01184, 0.01331, 0.01492, 0.01668, 0.01859, 0.02065, 0.02287, 0.02525, 0.02778, 0.03049, 0.03339, 0.03648, 0.03978, 0.04332)

qx3_crude <- dx3/Ex3

f3 <- function(p){
  sum(Ex3*(qx3_crude-p[1]-p[2]*qsx3)^2)
}

wls3 <- nlm(f3, c(0.0001, 0.0001))
wls3
```

Hence the weighted least squares estimates of a and b are respectively
```{r}
a3 <- wls3$estimate[1]
b3 <- wls3$estimate[2]

wls3$estimate
```

The graduated estimates are thus $q_x^g = a+bq_x^S$
```{r}
grad3 <- a3+b3*qsx3
grad3
```